{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "from nltk.corpus import brown as br\n",
    "import itertools\n",
    "from itertools import tee\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import udhr\n",
    "from nltk.metrics import spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Define a string s = 'colorless'. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colourless\n"
     ]
    }
   ],
   "source": [
    "s = \"colorless\"\n",
    "n_s = s[:4]+\"u\"+s[4:]\n",
    "print(n_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "☼ We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: dishes - without affix: dish\n",
      "String: running - without affix: run\n",
      "String: nationality - without affix: nation\n",
      "String: undo - without affix: un\n",
      "String: preheat - without affix: pre\n"
     ]
    }
   ],
   "source": [
    "strings = [\"dishes\", \"running\", \"nationality\", \"undo\", \"preheat\"]\n",
    "\n",
    "def rem_affix(str: s) -> str:\n",
    "    if s.endswith(\"es\"):\n",
    "        return s[:-2]\n",
    "    elif s.endswith(\"ing\"):\n",
    "        return s[:-4]\n",
    "    elif s.endswith(\"ality\"):\n",
    "        return s[:-5]\n",
    "    elif s.endswith(\"do\"):\n",
    "        return s[:-2]\n",
    "    elif s.endswith(\"heat\"):\n",
    "        return s[:-4]\n",
    "        \n",
    "for s in strings:\n",
    "    print(f\"String: {s} - without affix: {rem_affix(s)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "a\n",
      "e\n",
      "h\n",
      "e\n",
      "r\n",
      "p\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c6cb7ccfb298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# string \"preheat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# string \"preheat\"\n",
    "for char in range(len(s)+1):\n",
    "    print(s[::-1][char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "1. [a-zA-Z]+ # Minimun one lower- or uppercased char from the alphabet\n",
    "2. [A-Z][a-z]* # One uppercase char (followed by lowercase chars) \n",
    "3. p[aeiu]{,2}to # word beginning with char followed by one to max 2 from group ending with two: paato \n",
    "4. \\d+(\\.\\d+)? # Min one decimal (followed by dot and other decimal(s))\n",
    "5. ([^aeiou][aeiou][^aeiou])* non-vocale - vocale - nonvocale eg. markup\n",
    "6. \\w+|[^\\w\\s]+  Word or non-word/whitespace\n",
    "Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"☼1.3 Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL.\"\n",
    "nltk.re_show(\"\\w+|[^\\w\\s]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Write regular expressions to match the following classes of strings:\n",
    "A single determiner (assume that a, an, and the are the only determiners).\n",
    "An arithmetic expression using integers, addition, and multiplication, such as 2*3+8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"^(?:(?!a|an|are).)*\\s(a|an|are)\\s\" \n",
    "p2 = r\"\\d([\\*\\+\\/\\-]\\d)+\"\n",
    "calc = \"2*3+8/4-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.re_show(p2, calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_contents(url):\n",
    "    contents = request.urlopen(url).read().decode(\"utf8\")\n",
    "    return contents\n",
    "\n",
    "url_contents(\"http://facebook.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "Beim Stern, wenn er heruntersaust!\t[53:1]\n",
    "Euer Gefährte ist weder verwirrt, noch befindet er sich im Unrecht ,\t[53:2]\n",
    "noch spricht er aus Begierde.\t[53:3]\n",
    "Vielmehr ist es eine Offenbarung, die (ihm) eingegeben wird.\t[53:4]\n",
    "Gelehrt hat ihn einer, der über starke Macht verfügt ,\t[53:5]\n",
    "dessen Macht sich auf alles erstreckt; darum stand er aufrecht da ,\t[53:6]\n",
    "als er am obersten Horizont war.\t[53:7]\n",
    "Hierauf näherte er sich; kam dann nach unten ,\t[53:8]\n",
    "bis er eine Entfernung von zwei Bogenlängen erreicht hatte oder noch näher.\t[53:9]\n",
    "Und er offenbarte Seinem Diener, was er offenbarte.\t[53:10]\n",
    "(Und) dessen Herz hielt es nicht für gelogen, was er sah.\t[53:11]\n",
    "Wollt ihr da mit ihm über das streiten, was er sah?\t[53:12]\n",
    "Und er sah ihn bei einer anderen Begegnung\t[53:13]\n",
    "beim Lotusbaum am äußersten Ende ,\t[53:14]\n",
    "an dem das Paradies der Geborgenheit liegt.\t[53:15]\n",
    "Dabei überflutete den Lotusbaum, was (ihn) überflutete.\t[53:16]\n",
    "Da wankte der Blick nicht, noch schweifte er ab.\t[53:17]\n",
    "Wahrlich, er hatte eines der größten Zeichen seines Herrn gesehen.\t[53:18]\n",
    "Was haltet ihr nun von Al-Lat und Al-'Uzza\t[53:19]\n",
    "und Manah, der dritten der anderen?\t[53:20]\n",
    "Wie? Sollten euch die Knaben zustehen und Ihm die Mädchen?\t[53:21]\n",
    "Das wäre wahrhaftig eine unbillige Verteilung.\t[53:22]\n",
    "Wahrlich, es sind nur die Namen, die ihr euch ausgedacht habt - ihr und eure Väter -, für die Allah keinerlei Ermächtigung herabgesandt hat. Sie folgen einem bloßen Wahn und ihren persönlichen Neigungen, obwohl doch die Weisung ihres Herrn zu ihnen kam.\t[53:23]\n",
    "Kann der Mensch denn haben, was er nur wünscht?\t[53:24]\n",
    "Aber Allahs ist das Diesseits und das Jenseits.\t[53:25]\n",
    "Und so mancher Engel ist in den Himmeln, dessen Fürsprache nichts nützen wird, es sei denn, nachdem Allah dem die Erlaubnis (dazu) gegeben hat, dem Er will und der Ihm beliebt.\t[53:26]\n",
    "Solche, die nicht an das Jenseits glauben, benennen die Engel mit weiblichen Namen.\t[53:27]\n",
    "Jedoch sie besitzen kein Wissen hiervon. Sie gehen nur Vermutungen nach; und Vermutungen ersetzen nicht im geringsten die Wahrheit.\t[53:28]\n",
    "Darum wende dich von dem ab, der Unserer Ermahnung den Rücken kehrt und nichts als das Leben in dieser Welt begehrt.\t[53:29]\n",
    "Das ist die Summe ihres Wissens. Wahrlich, dein Herr kennt denjenigen recht wohl, der von Seinem Wege abirrt, und Er kennt auch jenen wohl, der den Weg befolgt.\t[53:30]\n",
    "Und Allahs ist, was in den Himmeln und was auf Erden ist, auf daß Er denen, die Böses tun, ihren Lohn für das gebe, was sie gewirkt haben; und auf daß Er die, die Gutes tun, mit dem Allerbesten belohne.\t[53:31]\n",
    "Jene, die die großen Sünden und Schändlichkeiten meiden - mit Ausnahme leichter Vergehen - wahrlich, dein Herr ist von weitumfassender Vergebung. Er kennt euch sehr wohl; als Er euch aus der Erde hervorbrachte, und als ihr Embryos in den Leibern eurer Mütter waret. Darum erklärt euch nicht selber als rein. Er kennt diejenigen am besten, die (Ihn) fürchten.\t[53:32]\n",
    "Siehst du den, der sich abkehrt\t[53:33]\n",
    "und wenig gibt und geizt?\t[53:34]\n",
    "Hat er wohl die Kenntnis des Verborgenen, so daß er es sehen könnte?\t[53:35]\n",
    "Oder ist ihm nicht erzählt worden, was in den Schriftblättern Moses' steht\t[53:36]\n",
    "und Abrahams, der (die Gebote) erfüllte?\t[53:37]\n",
    "(Geschrieben steht,) daß keine lasttragende (Seele) die Last einer anderen tragen soll ,\t[53:38]\n",
    "und daß dem Menschen nichts anderes zuteil wird als das, wonach er strebt ,\t[53:39]\n",
    "und daß sein Streben bald sichtbar wird.\t[53:40]\n",
    "Dann wird er dafür mit reichlichem Lohn belohnt werden.\t[53:41]\n",
    "Und (es steht geschrieben,) daß es bei deinem Herrn enden wird ,\t[53:42]\n",
    "und daß Er es ist, Der zum Lachen und Weinen bringt ,\t[53:43]\n",
    "und daß Er es ist, Der sterben läßt und lebendig macht ,\t[53:44]\n",
    "und daß Er die Paare (als) männliche und weibliche (Wesen) erschaffen hat\t[53:45]\n",
    "aus einem Samentropfen, der ausgestoßen wird ,\t[53:46]\n",
    "und daß Ihm die zweite Schöpfung obliegt ,\t[53:47]\n",
    "und daß Er allein reich und arm macht ,\t[53:48]\n",
    "und daß Er der Herr des Sirius ist ,\t[53:49]\n",
    "und daß Er die einstigen 'Ad vernichtete\t[53:50]\n",
    "und die Thamud, und keinen verschonte ,\t[53:51]\n",
    "und (daß Er) vordem das Volk Noahs (vernichtete) - wahrlich, sie waren höchst ungerecht und widerspenstig.\t[53:52]\n",
    "Und Er ließ die verderbten Städte einstürzen ,\t[53:53]\n",
    "so daß sie bedeckte, was (sie) bedeckte.\t[53:54]\n",
    "Welche Wohltaten deines Herrn willst du denn bestreiten?\t[53:55]\n",
    "Dies ist ein Warner wie die früheren Warner.\t[53:56]\n",
    "Die Stunde naht.\t[53:57]\n",
    "Keiner außer Allah kann sie abwenden.\t[53:58]\n",
    "Wundert ihr euch über diese Verkündigung?\t[53:59]\n",
    "Und ihr lacht; aber Weinen tut ihr nicht?\t[53:60]\n",
    "Und wollt ihr achtlos (hinsichtlich dieser Verkündigung) bleiben?\t[53:61]\n",
    "So fallt denn vor Allah anbetend nieder und dient (Ihm). .\t[53:62]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    file = open(f)\n",
    "    s = \"\"\n",
    "    for line in file:\n",
    "        s += line.replace(\"\\n\", \" \\n \")\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt(f, s):\n",
    "    with open(f, \"w\") as f:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_txt(\"corpus.txt\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "\n",
    "def tok_text(f) -> list:\n",
    "    tokenized = []\n",
    "    with open (f, \"r\", encoding=\"utf8\") as file:\n",
    "        pattern = r\"\"\"(?x) # set flag to allow verbose regexps\n",
    "        \\(?\\w+\\)?|\\[\\d+:\\d+\\]|[!;\\.,]\n",
    "        \"\"\"\n",
    "        tokenized = regexp_tokenize(file.read(), pattern)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_text(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns = [\"Raziye\", \"Johann-Sebastian\", \"40$\" \"12,34€\", \"350.00$\", \"04.04.2022\", \"April 4th, 2022\", \"04/04/22\", \"2022-04-04\"]\n",
    "tokenized = nltk.regexp_tokenize(\" \".join(proper_nouns), \"([A-Z]\\w+(-[A-Z]\\w+)*)|(\\d+([.,]+\\d*)*[$€])|((\\d+([\\./,-])*){,3})|[A-Z]\\w+\\s\\d+\\w\\w,\\s\\d+\")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.re_show(\"[A-Z]\\w+\\s\\d+\\w{,2},\\s\\d+\", \"April 4th, 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Rewrite the following loop as a list comprehension:\n",
    " \n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']  \n",
    "result = []  \n",
    "for word in sent:  \n",
    "> word_len = (word, len(word))  \n",
    "> result.append(word_len)  \n",
    "\n",
    "result  \n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = [(w, len(w)) for w in sent]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space, such as 's'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"Verily with every hardship will be ease.\"\n",
    "raw.split(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Write a for loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in raw:\n",
    "    print(char, end = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ What is the difference between calling split on a string with no argument or with ' ' as the argument, e.g. sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.split()) # splits by default on whitespaces (multiple, consecutive, \\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_n = raw[:6]+\"  \"+raw[6:]+\"\\t\\t\" # no difference\n",
    "print(raw_n.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in raw.split()]\n",
    "print(words.sort(),\"\\n\", words) #.sort() is in-place and sorts alphabetically starting with upper-case chars first\n",
    "words = [w for w in raw.split()]\n",
    "print(sorted(words),\"\\n\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Explore the difference between strings and integers by typing the following at a Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers using int(\"3\") and str(3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3\"*7, 3*7)\n",
    "print(type(str(3)), type(int(\"3\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ Use a text editor to create a file called prog.py containing the single line monty = 'Monty Python'. Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py part of the filename):\n",
    " \n",
    ">>> from prog import monty  \n",
    ">>> monty\n",
    "\n",
    "This time, Python should return with a value. You can also try import prog, in which case Python should be able to evaluate the expression prog.monty at the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prog import monty\n",
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prog\n",
    "prog.monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☼ What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello %.5s\" % monty) # displays first 5 chars\n",
    "print(\"Pi: %.2f\" % math.pi) # displays two digits after decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.Text(br.words(categories=\"lore\"))\n",
    "str_text = \" \".join(words)\n",
    "tokenized = nltk.word_tokenize(str_text)\n",
    "wh_words = [\"what\", \"when\", \"where\", \"who\", \"whom\", \"which\", \"whose\", \"why\", \"how\"]\n",
    "print(sorted([w for w in set(tokenized) for i in wh_words if w.startswith(f\"{i}\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "lines = open(\"freqs.py\").readlines()\n",
    "for line in lines:\n",
    "    l.append(line.split())\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokens.index(\"13\")) # Search with indexing\n",
    "\n",
    "def get_topdegree(city, tokens):\n",
    "    \"\"\"Returns forecast top temperature today for city\n",
    "    Params: city: str - the city of interest\n",
    "            tokens: list - the token range for html\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    url = \"https://www.wetter.de/deutschland/wetter-heidelberg-18224778.html?q=heidelberg\"\n",
    "    html = request.urlopen(url).read().decode(\"utf8\")\n",
    "    raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "    tokens = word_tokenize(raw)\n",
    "    print(f\"The top temperature in {city} for today: {tokens[8471]} is: {tokens[8473]}°C.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topdegree(\"Heidelberg\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown(url) -> list:\n",
    "    html = request.urlopen(url).read().decode(\"utf8\")\n",
    "    raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "    tokens = word_tokenize(raw)\n",
    "    lower_w = set([w for w in tokens if re.findall(r\"\\b[a-z]\\w*\", w)])\n",
    "    unknown = [w for w in lower_w if w not in nltk.corpus.words.words(\"en\")]\n",
    "    return unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(uknown: list):\n",
    "    categories = {\"en_words\":0, \"others\":0}\n",
    "    en_words, others = [], []\n",
    "    en_pattern = r\"\"\"(?x)\n",
    "    ^[a-z]+(?:-?[A-Z]*[a-z]+)*$ #| ^\\b[A-Z]?[a-z]+[\\.A-Z-]\n",
    "    \"\"\"\n",
    "    for w in unknown:\n",
    "        if re.search(en_pattern, w) or re.search(en_pattern, stem(w)):\n",
    "            categories[\"en_words\"]+=1\n",
    "            en_words.append(w)\n",
    "        else:\n",
    "            categories[\"others\"]+=1\n",
    "            others.append(w)\n",
    "    return categories, en_words, others\n",
    "        \n",
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unknown = unknown(\"https://www.nltk.org/book/ch03.html\")\n",
    "#categorize(unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unknown = unknown(\"http://news.bbc.co.uk/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats, en, oth = categorize(unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(r\"\\w+|n't\", \"don't\")) # splits only lately at apostrophe\n",
    "# Rather:\n",
    "print(re.findall(r\"^(do)(n't)\", \"don't\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(word: str) -> str:\n",
    "    rep = {\"a\":\"@\", \"e\":\"3\", \"i\":\"1\", \"o\":\"0\", \"u\":\"∪\", \"l\":\"|\", \"s\":\"5\", \".\":\"5w33t!\", \"ate\":\"8\"}\n",
    "    for k, v in rep.items():\n",
    "        word = word.replace(k, v)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(br.words(categories=\"science_fiction\"))\n",
    "hacker = [convert(w) for w in words]\n",
    "print(hacker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped(word: str) -> str:\n",
    "    if len(re.findall(r\"[a-zA-Z@∪|158]+\", word)) > 0:\n",
    "        word = re.sub(re.escape(word[0]), \"$\", word)\n",
    "        word = re.sub(re.escape(word[1:]), len(word[1:])*\"5\", word)\n",
    "    return word\n",
    "mapped_hacker = map(mapped, hacker)\n",
    "list(mapped_hacker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin  \n",
    "Write a function to convert a word to Pig Latin.  \n",
    "Write code that converts text, instead of individual words.  \n",
    "Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g. style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin(word) -> str:\n",
    "    match = re.search(r\"(?<!q)u|[AEIOUaeio]|(?<=[^AEIOUaeiou])y\", word) # Negative - Positive Lookbehind\n",
    "    if match is not None and match.start() > 0:\n",
    "        word += word[:match.start()]\n",
    "        pl_word = word.replace(f\"{word[:match.start()]}\", \"\", 1)\n",
    "        pl_word += \"ay\"\n",
    "        return pl_word\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig_latin(\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin(text: list) -> list:\n",
    "    pl_text = []\n",
    "    for word in text:\n",
    "        match = re.search(r\"(?<!q)u|[AEIOUaeio]|(?<=[^AEIOUaeiou])y\", word)\n",
    "        if match is not None and match.start() > 0:\n",
    "            word += word[:match.start()]\n",
    "            pl_word = word.replace(f\"{word[:match.start()]}\", \"\", 1)\n",
    "            pl_word += \"ay\"\n",
    "            pl_text.append(pl_word)\n",
    "        else:\n",
    "            pl_text.append(word)\n",
    "    return pl_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg as gt\n",
    "\n",
    "poems = list(gt.words(\"blake-poems.txt\"))\n",
    "pig_latin(poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hung_udhr = nltk.corpus.udhr.words(\"Hungarian_Magyar-Latin1\")\n",
    "vs = [(\"\".join(v).lower(), w) for w in hung_udhr for v in re.findall(r\"([AEIOUÖÜaeiouöü](?=\\w*[aeiouöü]|$))|((?<=[AEIOUÖÜaeiouöü]\\w*)[aeiouöü])\", w)]\n",
    "\n",
    "def pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "vow = []\n",
    "shared = \"\" # shared same word\n",
    "for w in vs:\n",
    "    if shared != w[1]:\n",
    "        vow.append([])\n",
    "        shared = w[1]\n",
    "        vow[-1].append(w[0])\n",
    "    else:\n",
    "        vow[-1].append(w[0])\n",
    "        shared = w[1]  #save current word \n",
    "\n",
    "vv_pairs = [list(pairwise(v)) for v in vow]\n",
    "vvs = [\"\".join(pair) for l in vv_pairs for pair in l]\n",
    "cfd = nltk.ConditionalFreqDist(vvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split() and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughter = \"\".join(random.choices(\"aehh \", k = 500))\n",
    "\"\".join(laughter.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53 +/- 0.15% is three words? Or should we say that it's a single compound word? Or should we say that it is actually nine words, since it's read \"four point five three, plus or minus zero point fifteen percent\"? Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if its one word → we're not interested in the numbers, only in the actual words, hence dictionary.\n",
    "if it's three words → could have anything to do with statistics.\n",
    "if it's more than that → speech KI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_s = br.words(categories = \"lore\"), br.sents(categories=\"lore\")\n",
    "j, j_s = br.words(categories = \"learned\"), br.sents(categories=\"learned\")\n",
    "\n",
    "cnt = 0\n",
    "for w in f:\n",
    "    cnt += len(w)\n",
    "fw_avg = round(cnt/len(f), 2)\n",
    "\n",
    "cnt = 0\n",
    "for w in j:\n",
    "    cnt += len(w)\n",
    "jw_avg = round(cnt/len(j), 2)\n",
    "\n",
    "cnt = 0\n",
    "for s in f_s:\n",
    "    cnt += len(s)\n",
    "fs_avg = round(cnt/len(f_s), 2)\n",
    "\n",
    "cnt = 0\n",
    "for s in j_s:\n",
    "    cnt += len(s)\n",
    "js_avg = round(cnt/len(j_s), 2)\n",
    "\n",
    "print(f\"Average word lenght in lore: {fw_avg}, in learned: {jw_avg}\")\n",
    "print(f\"Average sentence lenght in lore: {fs_avg}, in learned: {js_avg} \\n\")\n",
    "\n",
    "print(f\"Reading Difficulty for Lore as Automated Readability Index (ARI): {round(4.71*fw_avg + 0.5*fs_avg -21.43, 2)}\")\n",
    "print(f\"Reading Difficulty for Learned as Automated Readability Index (ARI): {round(4.71*jw_avg + 0.5*js_avg -21.43, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "words = br.words(categories=\"romance\")\n",
    "tokens = word_tokenize(\" \".join(words))\n",
    "\n",
    "p = [porter.stem(t) for t in tokens]\n",
    "l = [lancaster.stem(t) for t in tokens]\n",
    "distinct = list(set(l).symmetric_difference(set(p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'inexpressible'.index('re') # gets index of first letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Write code to convert nationality adjectives like Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_of_place_names\"\n",
    "\n",
    "html = request.urlopen(url).read().decode(\"utf8\")\n",
    "raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "tokens = [w for w in set(word_tokenize(raw)) if w.istitle()]\n",
    "\n",
    "def country(adj: str):\n",
    "    \"\"\"Returns the estimated according country name for the adjective\"\"\"\n",
    "    try:\n",
    "        diff = [set(adj).symmetric_difference(set(c)) if c != adj else set(adj).union(set(c)) for c in tokens] \n",
    "        index = diff.index(min(diff, key = lambda x: len(x)))\n",
    "        return tokens[index]\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "names = [country(adj) for adj in tokens]\n",
    "\n",
    "adjs = [\"Canadian\", \"American\", \"Australian\", \"German\", \"French\"] # doens't work in the other direction\n",
    "\n",
    "for a, c in zip(tokens, names):\n",
    "    if a in adjs:\n",
    "        print(f\"Country: {c} for Adjective: {a} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "◑ Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus and the findall() method for searching tokenized text described in 3.5. http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the discussion is about why in the English language we say \"as best as I can\" which we don't say for any other superlative. Let's investigate this in a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = nltk.Text(br.words())\n",
    "\n",
    "bw.findall(r\"<best> .* <can>\") # phrase is not apparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ An interesting challenge for tokenization is words that have been split across a line-break. E.g. if long-term is split, then we have the string long-\\nterm.\n",
    "Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the \\n character.\n",
    "Use re.sub() to remove the \\n character from these words.\n",
    "How might you identify words that should not remain hyphenated once the newline is removed, e.g. 'encyclo-\\npedia'?x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: search for partial words in dictionary, if apparent: split without hypephen\n",
    "text = \"\"\"long-\n",
    "term Encylo-\n",
    "pedia Social-\n",
    "media Party-\n",
    "ing\"\"\"\n",
    "\n",
    "new_t = \"\"\n",
    "\n",
    "for w in text.split(\" \"):\n",
    "    if wn.synsets(f\"{w.partition('-')[0]}\") and wn.synsets(f\"{w.partition('-')[2].lstrip()}\"):\n",
    "        new_t += re.sub(r\"(?<=[A-Za-z]+)-\\n(?=[A-Za-z]+)\", \"-\", w)\n",
    "        new_t += \" \"\n",
    "    else:\n",
    "        new_t += re.sub(r\"(?<=[A-Za-z]+)-\\n(?=[A-Za-z]+)\", \"\", w)\n",
    "        new_t += \" \"\n",
    "        \n",
    "print(f\"Old: {repr(text)} \\n and new: {repr(new_t)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g. compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import abc\n",
    "rural = abc.raw(\"rural.txt\")\n",
    "science = abc.raw(\"science.txt\")\n",
    "token_r = nltk.word_tokenize(rural)\n",
    "token_s = nltk.word_tokenize(science)\n",
    "texts = {\"rural\": token_r, \"science\": token_s}\n",
    "\n",
    "def ARI(text):\n",
    "    punct = [\"?\", \"!\", \".\"]\n",
    "\n",
    "    w_cnt, w_len, s_cnt = 0, 0, 0\n",
    "\n",
    "    for w in text:\n",
    "        if w not in punct:\n",
    "            w_cnt += 1\n",
    "            w_len += len(w)\n",
    "        else:\n",
    "            s_cnt += 1\n",
    "\n",
    "    my_s = w_cnt/s_cnt\n",
    "    my_w = w_len/len(token_r)\n",
    "    return my_w, my_s\n",
    "\n",
    "for k, v in texts.items():\n",
    "    my_w, my_s = ARI(v)\n",
    "    print(f\"Reading Difficulty as ARI for ABC {k} news: {round(4.71*my_w + 0.5*my_s -21.43, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ Rewrite the following nested loop as a nested list comprehension:\n",
    " \n",
    "words = ['attribution', 'confabulation', 'elocution','sequoia', 'tenacious', 'unidirectional']  \n",
    "vsequences = set()  \n",
    "for word in words:  \n",
    ">vowels = []  \n",
    ">for char in word:  \n",
    ">>if char in 'aeiou':  \n",
    ">>>vowels.append(char)\n",
    ">\n",
    ">vsequences.add(''.join(vowels))   \n",
    "\n",
    "sorted(vsequences)  \n",
    "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution','sequoia', 'tenacious', 'unidirectional']\n",
    "# for each word get its vowels and store in sorted set\n",
    "vseq = [\"\".join([char for char in word if char in \"aeiou\"])for word in words]\n",
    "vseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ Use WordNet to create a semantic index for a text collection. Extend the concordance search program in 3.6, indexing each word using the offset of its first synset, e.g. wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "        try:\n",
    "            self._further_index = nltk.Index((self._offset(word), i) \n",
    "                                 for (i, word) in enumerate(text))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "        try:\n",
    "            hypo = self._offset(word)\n",
    "            for j in self._further_index[hypo]:\n",
    "                lcontext = ' '.join(self._text[j-wc:j])\n",
    "                rcontext = ' '.join(self._text[j:j+wc])\n",
    "                ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "                rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "                print(ldisplay, rdisplay)\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "    \n",
    "    def _offset(self, word):\n",
    "        try:\n",
    "            hypos = wn.synsets(word)[0].hyponyms()\n",
    "            return [lemma.name() for synset in hypos for lemma in synset.lemmas()][0] # get first hyponym\n",
    "        except:\n",
    "            pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('animal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), and NLTK's frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FreqDist for unigrams. Rank Correl. for proportion of unigrams.\n",
    "    1st step: Get numbers(unigrams: set of all chars in words) for each lang from udhr\n",
    "    2nd step: Get numbers from text\n",
    "    3rd step: Compare \n",
    "'''\n",
    "\n",
    "languages = [\"Uighur_Uyghur-Latin1\", \"Trukese_Chuuk-Latin1\", \n",
    "             \"Romani-Latin1\", \"Kurdish-UTF8\", \"Farsi_Persian-UTF8\", \"Albanian_Shqip-Latin1\"]\n",
    " \n",
    "cfd = nltk.ConditionalFreqDist( # FreqDist of single characters in each language\n",
    "            (lang, char)\n",
    "            for lang in languages\n",
    "            for word in udhr.words(lang) \n",
    "            for char in word.lower() if char.isalpha()) # only allow alphabetics\n",
    "\n",
    "def guess(text):\n",
    "    \"\"\"Guesses the language of the given text\"\"\"\n",
    "    \n",
    "    fdist = nltk.FreqDist(char.lower() for w in text for char in w)\n",
    "    d = {\"Unknown\":fdist}\n",
    "    cfd.update(d)\n",
    "\n",
    "    scores = {}\n",
    "    for lang in languages:\n",
    "        scores[lang] = spearman.spearman_correlation(cfd.get(lang), cfd.get(\"Unknown\"))\n",
    "    \n",
    "    print(f\"The guess for the text language is: {max(scores, key = scores.get)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = udhr.words(\"Turkish_Turkce-Turkish\") \n",
    "guess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. (Note that this is a crude approach; doing it well is a difficult, open research problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novelty(text):\n",
    "    novelties = []\n",
    "    oldies = []\n",
    "    sims = {}\n",
    "    for sent in text:\n",
    "        for w in sent:\n",
    "            for s in wn.synsets(w.lower()):\n",
    "                 sims[w] = set([s.path_similarity(x) for j in sent for x in wn.synsets(j.lower()) if j != w]) # no sim to itself\n",
    "    for k, v in sims.items():\n",
    "        n = list(filter(None, v))\n",
    "        if n:\n",
    "            sims[k] = round(max(n), 2) # rather than averaging, taking max value (closest path) seems more intelligible\n",
    "            if sims[k] < 0.09:\n",
    "                novelties.append(k)\n",
    "            elif sims[k] > 0.25:\n",
    "                oldies.append(k)\n",
    "    return set(novelties), set(oldies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(br.sents(categories=\"news\"))[:500]\n",
    "novels, olds = novelty(text)\n",
    "print(f\"Approximated novelties: {novels} \\n\\n vs. approximated oldies: {olds}\") # seems arbitrary.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ Read the article on normalization of non-standard words (Sproat et al, 2001), and implement a similar system for text normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](normal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
